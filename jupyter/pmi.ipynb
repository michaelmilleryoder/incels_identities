{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c619ab-2073-4c1b-b9db-451b49207fc7",
   "metadata": {},
   "source": [
    "# Run PMI to find most associated words with identity terms in each corpus split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fe0dd-3241-453d-8dfe-911b9d56995c",
   "metadata": {},
   "source": [
    "## Load, process data, calculate cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19df8ba1-4c63-449a-a204-577d8024389f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0_anti-bot_sents', '0_anti-human_sents', '0_pro-bot_sents', '0_pro-human_sents'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data (tweet texts)\n",
    "import os\n",
    "\n",
    "split_type = '0_pro_anti_bot_human'\n",
    "dirpath = os.path.join('/home/huixiann/2022_socialbias_vaccine/michael/SAGE/py-sage/input/', split_type)\n",
    "processed = {}\n",
    "\n",
    "def remove_tok(tok):\n",
    "    return tok.startswith('http') or tok.startswith('#') or tok.isnumeric()\n",
    "\n",
    "for fname in sorted(os.listdir(dirpath)):\n",
    "    fpath = os.path.join(dirpath, fname)\n",
    "    with open(fpath, 'r') as f:\n",
    "        processed[fname.split('.')[0]] = [[tok for tok in doc.split() if not remove_tok(tok)] for doc in f.read().splitlines()]\n",
    "        \n",
    "processed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d513be-67ec-4576-8a5e-7a5724c65ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce60b632fc844c3e9b6c54fcacc10145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:10<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_anti-bot_sents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd0253f4ffe48999c30ced2f2a27665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_anti-human_sents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1074c951f5374e8586e07ef82917097a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2018806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_pro-bot_sents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8890c526a44714bb6f03742a81d15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1921921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_pro-human_sents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4afff2bdd2d41e4958ca3877e7b1087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3042223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build dictionary of raw word co-occurrences (co-occur if the words occur in the same document)\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "freq_threshold = 1000\n",
    "\n",
    "def process_section(section_parts):\n",
    "    name, section = section_parts\n",
    "    print(name)\n",
    "    sec_cooccurrences = defaultdict(int) # (word1, word2): n_times_co-occurs\n",
    "    sec_combination_word_freqs = defaultdict(int) # word1: n_times_occurs_in_combinations\n",
    "    sec_total_combinations = 0\n",
    "\n",
    "    counts = Counter([w for doc in section for w in doc])\n",
    "    sec_word_freqs = Counter({k: c for k,c in counts.items() if c >= freq_threshold})\n",
    "    \n",
    "    for doc in tqdm(section):\n",
    "        doc_toks = [w for w in doc if w in sec_word_freqs] # filters by freq\n",
    "\n",
    "        for pair in list(itertools.combinations(doc_toks, 2)):\n",
    "            sec_cooccurrences[tuple(sorted(pair))] += 1\n",
    "            sec_combination_word_freqs[pair[0]] += 1\n",
    "            sec_combination_word_freqs[pair[1]] += 1\n",
    "            sec_total_combinations += 1\n",
    "            \n",
    "    return name, sec_cooccurrences, sec_word_freqs, sec_combination_word_freqs, sec_total_combinations\n",
    "\n",
    "with Pool(len(processed)) as p:\n",
    "    results = list(tqdm(p.imap(process_section, sorted(processed.items())), total=len(processed)))\n",
    "\n",
    "cooccurrences = {}\n",
    "word_freqs = {}\n",
    "combination_word_freqs = {}\n",
    "total_combinations = {}\n",
    "\n",
    "for result in results:\n",
    "    name, sec_cooccurrences, sec_word_freqs, sec_combination_word_freqs, sec_total_combinations = result    \n",
    "    cooccurrences[name] = sec_cooccurrences\n",
    "    word_freqs[name] = sec_word_freqs\n",
    "    combination_word_freqs[name] = sec_combination_word_freqs\n",
    "    total_combinations[name] = sec_total_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4158faa8-6611-4530-aadc-0c8c9f59a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def pmi(words, word_freqs, cooccurrences, n):\n",
    "    numerator = n * cooccurrences[words]\n",
    "    if numerator == 0:\n",
    "        return 0\n",
    "    denominator = word_freqs[words[0]] * word_freqs[words[1]]\n",
    "    return math.log(numerator/denominator, 2)\n",
    "\n",
    "def top_pmi(word, word_freqs, cooccurrences, n):\n",
    "    # Returns top co-occurring words with a specified word based on PMI\n",
    "    \n",
    "    cooccurring_words = []\n",
    "    \n",
    "    pairs = [pair for pair in cooccurrences.keys() if word in pair and pair != (word, word)]  # all words that co-occur\n",
    "    \n",
    "    for pair in pairs:\n",
    "        other_word = [w for w in pair if w != word][0]\n",
    "        cooccurring_words.append((other_word, pmi(pair, word_freqs, cooccurrences, n)))\n",
    "        \n",
    "    return sorted(cooccurring_words, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559becf6-49f8-4d96-b289-af966d46ce7a",
   "metadata": {},
   "source": [
    "## View top associated terms with terms of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dad3fb-4873-4659-bf3b-beb284b9ea7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gender/sexuality', 'age', 'race/ethnicity/nationality', 'religion', 'class', 'medical'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load identity terms (terms of interest)\n",
    "import json\n",
    "\n",
    "identities_fpath = '../identities.json'\n",
    "with open(identities_fpath) as f:\n",
    "    identities = json.load(f)\n",
    "identities.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128f374a-4884-45db-9d5c-885f8dc6f154",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american\n",
      "0_anti-bot_sents: neighbors, latin, yourself, ye, cuba\n",
      "0_anti-human_sents: cuba's, praised, latin, cuba, neighbors\n",
      "0_pro-bot_sents: rescue, weird, neighbors, exchange, à¸«à¸™\n",
      "0_pro-human_sents: buildi, assets, reserves, weird, embassy\n",
      "\n",
      "americans\n",
      "0_anti-bot_sents: worried, missed, black, database, killed\n",
      "0_anti-human_sents: extend, cards, worried, easier, obligation\n",
      "0_pro-bot_sents: aug, racial, snapshot, miles, easier\n",
      "0_pro-human_sents: aug, lagging, safer, hrs, racial\n",
      "\n",
      "old\n",
      "0_anti-bot_sents: mom, nurse, conditions, mother, healthy\n",
      "0_anti-human_sents: club, student, volunteer, mom, nurse\n",
      "0_pro-bot_sents: struggle, toronto, east, stealing, admits\n",
      "0_pro-human_sents: 16-59, minutes, toronto, east, kagan\n",
      "\n",
      "children\n",
      "0_anti-bot_sents: zydus, ages, ðŸ¤¬, st, started\n",
      "0_anti-human_sents: embarrassing, inject, name, perspective, vector\n",
      "0_pro-bot_sents: ages, decline, upcoming, young, services\n",
      "0_pro-human_sents: zydus, aborted, lifesaving, decline, plea\n",
      "\n",
      "indian\n",
      "0_anti-bot_sents: troopers, army, specific, material, urgently\n",
      "0_anti-human_sents: army, material, specific, sources, urgently\n",
      "0_pro-bot_sents: army, procure, letter, belgium, highly\n",
      "0_pro-human_sents: capable, belgium, procure, letter, army\n",
      "\n",
      "indians\n",
      "0_anti-bot_sents: meanwhile, later, cr, build, pakistan\n",
      "0_anti-human_sents: refuses, successfully, build, later, cr\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "young\n",
      "0_anti-bot_sents: acknowledges, discontinue, heart, inflammation, risks\n",
      "0_anti-human_sents: â‰¤, acknowledges, discontinue, risks, heart\n",
      "0_pro-bot_sents: staying, remarkable, heart, senior, year-old\n",
      "0_pro-human_sents: midnight, appointmen, staying, remarkable, heart\n",
      "\n",
      "white\n",
      "0_anti-bot_sents: house, nyc, game, briefing, black\n",
      "0_anti-human_sents: house, nyc, donating, game, black\n",
      "0_pro-bot_sents: blocks, house, stricter, previously, blocked\n",
      "0_pro-human_sents: blocks, stricter, house, latino, previously\n",
      "\n",
      "man\n",
      "0_anti-bot_sents: harm, arrested, police, propaganda, concerns\n",
      "0_anti-human_sents: harm, arrested, police, speaks, closed\n",
      "0_pro-bot_sents: staying, remarkable, transporting, year-old, young\n",
      "0_pro-human_sents: midnight, appointmen, tw, unfortunately, staying\n",
      "\n",
      "black\n",
      "0_anti-bot_sents: communities, gap, opened, places, lies\n",
      "0_anti-human_sents: experiment, trusted, populations, indigenous, publix\n",
      "0_pro-bot_sents: latino, skepticism, mit, racial, pressure\n",
      "0_pro-human_sents: diabetes, latino, lagging, skepticism, asian\n",
      "\n",
      "poor\n",
      "0_anti-bot_sents: mike, merck, nations, rich, wealthy\n",
      "0_anti-human_sents: wealthy, publix, mike, nations, rich\n",
      "0_pro-bot_sents: merck, rich, shut, nations, publix\n",
      "0_pro-human_sents: merck, early-stage, life-saving, doh, sees\n",
      "\n",
      "elderly\n",
      "0_anti-bot_sents: incredible, complete, vulnerable, dying, appears\n",
      "0_anti-human_sents: incredible, influenza, association, complete, vulnerable\n",
      "0_pro-bot_sents: vulnerable, frontline, individuals, homes, town\n",
      "0_pro-human_sents: bcg, vulnerable, incentive, frontline, homes\n",
      "\n",
      "rich\n",
      "0_anti-bot_sents: poor, nations, countries, rolling, hoarding\n",
      "0_anti-human_sents: shocking, poor, nations, urging, equity\n",
      "0_pro-bot_sents: jump, poor, gap, disaster, slots\n",
      "0_pro-human_sents: achieve, hoarding, poorer, â–ª, ourselves\n",
      "\n",
      "rural\n",
      "0_anti-bot_sents: cover, areas, borders, rising, spike\n",
      "0_anti-human_sents: nepal, areas, cover, borders, area\n",
      "0_pro-bot_sents: areas, began, counties, preparing, mobile\n",
      "0_pro-human_sents: physicians, biontech's, areas, began, suspended\n",
      "\n",
      "urban\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "mom\n",
      "0_anti-bot_sents: hoax, lost, nurse, conditions, friends\n",
      "0_anti-human_sents: hoax, placebo, nurse, self, lost\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: randomized, self, placebo, volunteered, either\n",
      "\n",
      "youth\n",
      "0_anti-bot_sents: future, modi, ji, yrs, mobile\n",
      "0_anti-human_sents: 8:, hosting, various, modi, attend\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: session, attend, network, consent, defeat\n",
      "\n",
      "father\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: eric, malaria, refused, worked, wife\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at PMI for high-frequency terms\n",
    "high_freq_terms = [\n",
    "    'american', \n",
    "    'americans',\n",
    "    'old',\n",
    "    'children',\n",
    "    'indian',\n",
    "    'indians',\n",
    "    'young',\n",
    "    'white',\n",
    "    'man',\n",
    "    'black',\n",
    "    'poor',\n",
    "    'elderly',\n",
    "    'rich',\n",
    "    'rural',\n",
    "    'urban',\n",
    "    'mom',\n",
    "    'youth', \n",
    "    'father'\n",
    "]\n",
    "for term in high_freq_terms:\n",
    "    print(term)\n",
    "    for name in processed:\n",
    "        outstring = ', '.join([el[0] for el in top_pmi(term, combination_word_freqs[name], cooccurrences[name], total_combinations[name])[:5]])\n",
    "        print(f'{name}: {outstring}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0659d038-4631-4622-a670-50133c023298",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender/sexuality\n",
      "woman\n",
      "0_anti-bot_sents: paused, dies, goes, condition, year-old\n",
      "0_anti-human_sents: paused, suspected, condition, oxford-astrazeneca, goes\n",
      "0_pro-bot_sents: significant, inside, role, pharmacy, designed\n",
      "0_pro-human_sents: saving, phenomenal, significant, role, st\n",
      "\n",
      "women\n",
      "0_anti-bot_sents: pregnant, safely, warns, recommends, misinformation\n",
      "0_anti-human_sents: brillian, fear-mongering, pregnant, breastfeeding, nursing\n",
      "0_pro-bot_sents: pregnant, men, benefits, nor, raise\n",
      "0_pro-human_sents: pregnant, determine, raise, benefits, space\n",
      "\n",
      "man\n",
      "0_anti-bot_sents: harm, arrested, police, propaganda, concerns\n",
      "0_anti-human_sents: harm, arrested, police, speaks, closed\n",
      "0_pro-bot_sents: staying, remarkable, transporting, year-old, young\n",
      "0_pro-human_sents: midnight, appointmen, tw, unfortunately, staying\n",
      "\n",
      "men\n",
      "0_anti-bot_sents: billionaires, became, vi, industry, lockdowns\n",
      "0_anti-human_sents: poverty, whic, became, billionaires, vi\n",
      "0_pro-bot_sents: remarkable, gap, they're, poll, whole\n",
      "0_pro-human_sents: billionaires, remarkable, gap, vi, det\n",
      "\n",
      "girl\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "girls\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "gal\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "boy\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "boys\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "mother\n",
      "0_anti-bot_sents: child, tweet, she, old, 45,18\n",
      "0_anti-human_sents: wasn't, tweet, she, infected, california\n",
      "0_pro-bot_sents: killed, kill, year-old, her, wanted\n",
      "0_pro-human_sents: afraid, killed, kill, year-old, wit\n",
      "\n",
      "mum\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "mom\n",
      "0_anti-bot_sents: hoax, lost, nurse, conditions, friends\n",
      "0_anti-human_sents: hoax, placebo, nurse, self, lost\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: randomized, self, placebo, volunteered, either\n",
      "\n",
      "mama\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "father\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: eric, malaria, refused, worked, wife\n",
      "\n",
      "dad\n",
      "0_anti-bot_sents: load, vaxed, viral, north, ago\n",
      "0_anti-human_sents: vaxed, load, daughter, viral, seeks\n",
      "0_pro-bot_sents: vaxed, load, icu, malaysian, fi\n",
      "0_pro-human_sents: load, vaxed, malaysian, delhi, viral\n",
      "\n",
      "papa\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "trans\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "transgender\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "queer\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "gay\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "lesbian\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "bisexual\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "age\n",
      "children\n",
      "0_anti-bot_sents: zydus, ages, ðŸ¤¬, st, started\n",
      "0_anti-human_sents: embarrassing, inject, name, perspective, vector\n",
      "0_pro-bot_sents: ages, decline, upcoming, young, services\n",
      "0_pro-human_sents: zydus, aborted, lifesaving, decline, plea\n",
      "\n",
      "kid\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "child\n",
      "0_anti-bot_sents: mother, juin, 45,18, benefit, personal\n",
      "0_anti-human_sents: personal, kills, benefit, juin, begins\n",
      "0_pro-bot_sents: healthy, missed, palestinian, israeli, killed\n",
      "0_pro-human_sents: forces, healthy, attend, missed, routine\n",
      "\n",
      "young\n",
      "0_anti-bot_sents: acknowledges, discontinue, heart, inflammation, risks\n",
      "0_anti-human_sents: â‰¤, acknowledges, discontinue, risks, heart\n",
      "0_pro-bot_sents: staying, remarkable, heart, senior, year-old\n",
      "0_pro-human_sents: midnight, appointmen, staying, remarkable, heart\n",
      "\n",
      "youth\n",
      "0_anti-bot_sents: future, modi, ji, yrs, mobile\n",
      "0_anti-human_sents: 8:, hosting, various, modi, attend\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: session, attend, network, consent, defeat\n",
      "\n",
      "old\n",
      "0_anti-bot_sents: mom, nurse, conditions, mother, healthy\n",
      "0_anti-human_sents: club, student, volunteer, mom, nurse\n",
      "0_pro-bot_sents: struggle, toronto, east, stealing, admits\n",
      "0_pro-human_sents: 16-59, minutes, toronto, east, kagan\n",
      "\n",
      "elderly\n",
      "0_anti-bot_sents: incredible, complete, vulnerable, dying, appears\n",
      "0_anti-human_sents: incredible, influenza, association, complete, vulnerable\n",
      "0_pro-bot_sents: vulnerable, frontline, individuals, homes, town\n",
      "0_pro-human_sents: bcg, vulnerable, incentive, frontline, homes\n",
      "\n",
      "aged\n",
      "0_anti-bot_sents: zydus, germany's, book, july, life-saving\n",
      "0_anti-human_sents: germany's, 8:, july, life-saving, book\n",
      "0_pro-bot_sents: georgians, georgia, thursday, beginning, book\n",
      "0_pro-human_sents: georgians, georgia, zydus, thursday, beginning\n",
      "\n",
      "grandma\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "grandpa\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "grandfather\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "grandmother\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "race/ethnicity/nationality\n",
      "indian\n",
      "0_anti-bot_sents: troopers, army, specific, material, urgently\n",
      "0_anti-human_sents: army, material, specific, sources, urgently\n",
      "0_pro-bot_sents: army, procure, letter, belgium, highly\n",
      "0_pro-human_sents: capable, belgium, procure, letter, army\n",
      "\n",
      "indians\n",
      "0_anti-bot_sents: meanwhile, later, cr, build, pakistan\n",
      "0_anti-human_sents: refuses, successfully, build, later, cr\n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "asian\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: mit, covering, genetic, southeast, black\n",
      "\n",
      "asians\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "black\n",
      "0_anti-bot_sents: communities, gap, opened, places, lies\n",
      "0_anti-human_sents: experiment, trusted, populations, indigenous, publix\n",
      "0_pro-bot_sents: latino, skepticism, mit, racial, pressure\n",
      "0_pro-human_sents: diabetes, latino, lagging, skepticism, asian\n",
      "\n",
      "blacks\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "white\n",
      "0_anti-bot_sents: house, nyc, game, briefing, black\n",
      "0_anti-human_sents: house, nyc, donating, game, black\n",
      "0_pro-bot_sents: blocks, house, stricter, previously, blocked\n",
      "0_pro-human_sents: blocks, stricter, house, latino, previously\n",
      "\n",
      "whites\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "american\n",
      "0_anti-bot_sents: neighbors, latin, yourself, ye, cuba\n",
      "0_anti-human_sents: cuba's, praised, latin, cuba, neighbors\n",
      "0_pro-bot_sents: rescue, weird, neighbors, exchange, à¸«à¸™\n",
      "0_pro-human_sents: buildi, assets, reserves, weird, embassy\n",
      "\n",
      "americans\n",
      "0_anti-bot_sents: worried, missed, black, database, killed\n",
      "0_anti-human_sents: extend, cards, worried, easier, obligation\n",
      "0_pro-bot_sents: aug, racial, snapshot, miles, easier\n",
      "0_pro-human_sents: aug, lagging, safer, hrs, racial\n",
      "\n",
      "african\n",
      "0_anti-bot_sents: south, neutralizing, ðŸ‡­, created, world's\n",
      "0_anti-human_sents: nonstop, affairs, radio, music, playing\n",
      "0_pro-bot_sents: ðŸ‡­, oxford-astrazeneca, south, neutralizing, 600,000\n",
      "0_pro-human_sents: 49.4, mutant, varia, uganda, meaning\n",
      "\n",
      "religion\n",
      "christian\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "christians\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "catholic\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "catholics\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "muslim\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: couple, supports, investigation, australian, discussing\n",
      "\n",
      "muslims\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "jew\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "jews\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "class\n",
      "rich\n",
      "0_anti-bot_sents: poor, nations, countries, rolling, hoarding\n",
      "0_anti-human_sents: shocking, poor, nations, urging, equity\n",
      "0_pro-bot_sents: jump, poor, gap, disaster, slots\n",
      "0_pro-human_sents: achieve, hoarding, poorer, â–ª, ourselves\n",
      "\n",
      "poor\n",
      "0_anti-bot_sents: mike, merck, nations, rich, wealthy\n",
      "0_anti-human_sents: wealthy, publix, mike, nations, rich\n",
      "0_pro-bot_sents: merck, rich, shut, nations, publix\n",
      "0_pro-human_sents: merck, early-stage, life-saving, doh, sees\n",
      "\n",
      "rural\n",
      "0_anti-bot_sents: cover, areas, borders, rising, spike\n",
      "0_anti-human_sents: nepal, areas, cover, borders, area\n",
      "0_pro-bot_sents: areas, began, counties, preparing, mobile\n",
      "0_pro-human_sents: physicians, biontech's, areas, began, suspended\n",
      "\n",
      "urban\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "medical\n",
      "immunocompromised\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n",
      "diabetes\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: pressure, matter, blood, means, priority\n",
      "\n",
      "heart disease\n",
      "0_anti-bot_sents: \n",
      "0_anti-human_sents: \n",
      "0_pro-bot_sents: \n",
      "0_pro-human_sents: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top associated words with all terms\n",
    "for cat in identities:\n",
    "    print(cat)\n",
    "    for term in identities[cat]:\n",
    "        print(term)\n",
    "        for name in processed:\n",
    "            outstring = ', '.join([el[0] for el in top_pmi(term, combination_word_freqs[name], cooccurrences[name], total_combinations[name])[:5]])\n",
    "            print(f'{name}: {outstring}')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
